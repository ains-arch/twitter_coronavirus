# Coronavirus on Twitter in 2020

In this project, I scanned all geotagged tweets posted in 2020 to analyze the spread of coronavirus information on social media.
This project allowed me to:

1. work with large scale datasets
1. work with multilingual text
1. use the MapReduce divide-and-conquer paradigm to create parallel code

## Background

About 2% of the 500 million tweets sent everyday are geotagged.
For this project, I started with a dataset of about 1.1 billion tweets - all geotagged tweets that were sent in 2020 - as well as some starter functions for processing them.

I followed the MapReduce procedure to analyze these tweets.
The partition step was already done in the creation of the dataset as the tweets are already split into one file per day.
I implemented the map and reduce steps by writing original commands and by tweaking the starter code.

## Method

### Mapping and visualizing a single day

The starter `map.py` file processed the zip file for an individual day.
I started by using this command to process all of the tweets sent on Febuary 16th.

The starter `visualize.py` file printed the output of running the `map.py` file.
I used this command to display the total number of times the hashtag `#coronavirus` was used on February 16th in each of the languages supported by Twitter.

### Reducing and visualizing multiple days

The starter `reduce.py` file merges the outputs generated by the `map.py` file so that the combined files could be visualized.
I generated a new output file for February 17th and visualized it by merging it into a single file with `reduce.py`.

### Modifying the mapper and reducing the outputs

I modified the `map.py` file so that it tracks the usage of the hashtags on both a language and country level.
I then created a shell script called `run_maps.sh` that loops over each file in the dataset and runs the `map.py` command on that file.
To speed up execution, I ran the `map.py` commands in parallel.

I used `reduce.py` to combine all of the language files into a single file, and all of the country files into a different file.

### Visualizing the full dataset

I modified the `visualize.py` file so that, rather than simply printing the results, it now generates a bar graph of the results and stores the bar graph as a png file using the matplotlib library.
I used this command to create the following bar graphs of the top 10 languages and countries by volume of tweets with the given hashtag:

<img src=reduced.lang_%23coronavirus_figure.png width=100% />

<img src=reduced.country_%23coronavirus_figure.png width=100% />

<img src=reduced.lang_%23코로나바이러스_figure.png width=100% />

<img src=reduced.country_%23코로나바이러스_figure.png width=100% />

By "Korean Coronavirus Hashtag", I am referring specifically to `#코로나바이러스`.
`und`, or "undefined" tweets are tweets that don't have language metadata.

I created a new visualization command from scratch, called `alternative_reduce.py`, that takes as input on the command line a list of hashtags,
and outputs a line plot.
I used this command to create the following plot:

<img src=alternative.png width=100% />

## Conclusion and next steps

While the initial four data visualizations don't reveal much - `#coronavirus` was used mainly in English tweets from English-speaking countries, `#코로나바이러스` was used less frequently and almost exclusively in Korea - they did allow me to verify that the MapReduce steps produced an output that made sense.

The visualization I created from scratch revealed more interesting insights.
Tweets containing the relevant hashtags first appeared with any regularity in February, before sharply increasing in volume in March.
This tracks with what I'd expect from my own experience of the coronavirus news cycle. Coronavirus first began being discussed in the mainstream in February when it first started gaining significant ground in China
then slowed until, seemingly over the course of a single week in March, it really hit the US and caused the first lockdowns.

This visualization also suggest something else I'd noticed in public discussions of coronavirus. Initially, the words "corona" and "coronavirus" were used mostly interchangeably in the early days of the pandemic, but over time, "COVID-19" became the more common name.

Interestingly, the volume of tweets doesn't follow the [US daily cases](https://ourworldindata.org/explorers/coronavirus-data-explorer?zoomToSelection=true&time=earliest..2020-12-30&facet=none&country=~USA&pickerSort=desc&pickerMetric=new_cases_smoothed_per_million&Metric=Confirmed+cases&Interval=New+per+day&Relative+to+Population=true&Color+by+test+positivity=false) graph, or the worldwide cases graph for that matter, beyond the initial uptick in March.
This suggests that even though the coronavirus caseload was worsening, it was discussed less and less once it had been around for a while.

This is, of course, mainly conjecture based on my own subjective experience of the pandemic (and Twitter). Were I to look into further work with this dataset or similar datasets, I would start with casting a wider net with the hashtags. The plain "#covid" hashtag is notably missing, and it would be interesting to see how its use differed from the other hashtags.
I would also take into account more how Twitter treats hashtags - it makes sense that "#covid-19" had barely any use comparatively, as including a dash breaks some of the hashtag's functionality.
While it would be a much larger dataset and therefore likely more difficult to work with, if it would be possible to investigate all tweets, rather than just the geotagged ones, that would provide a clearer picture of what discussion was actually taking place.
Rather than relying on hashtags as a proxy for tweet content, it might also be helpful to directly process the content of the tweet for coronavirus keywords, as hashtags are not often used in casual discussion on Twitter.

Ultimately, I consider this project a success as it allowed me to develop my data analysis and visualization skills using a dataset I'm interested in.
